#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Quantitative Results Generator for AdaptiveISP Chapter 7.3

This script generates the quantitative results shown in section 3.2 of the subchapter.
It provides detailed performance analysis across multiple test scenarios and generates
comprehensive statistical reports.

Usage:
    python generate_quantitative_results.py
    python generate_quantitative_results.py --output_dir results_analysis
    python generate_quantitative_results.py --scenarios 10 --iterations 5
"""

import os
import sys
import cv2
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
import statistics

# Add current directory to path
sys.path.append('.')

from isp_pipeline_demo import (
    ISPPipeline, ExposureModule, GammaModule, WhiteBalanceModule,
    DenoiseModule, SharpenModule, ContrastModule, SaturationModule,
    create_baseline_pipeline, create_adaptive_pipeline
)
from performance_analyzer import (
    ISPAnalyzer, ImageQualityAnalyzer, DetectionAnalyzer, PerformanceAnalyzer
)


class QuantitativeResultsGenerator:
    """Generate comprehensive quantitative results for AdaptiveISP analysis"""
    
    def __init__(self, output_dir: str = "quantitative_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize analyzers
        self.isp_analyzer = ISPAnalyzer()
        self.image_quality_analyzer = ImageQualityAnalyzer()
        self.detection_analyzer = DetectionAnalyzer()
        self.performance_analyzer = PerformanceAnalyzer()
        
        # Initialize ISP pipelines
        self.traditional_pipeline = create_baseline_pipeline()
        self.adaptive_pipeline = create_adaptive_pipeline()
        
        # Results storage
        self.results = []
        self.statistical_summary = {}
        
    def create_test_scenarios(self, num_scenarios: int = 4) -> Dict[str, np.ndarray]:
        """Create multiple test scenarios for comprehensive evaluation"""
        scenarios = {}
        
        # Scenario 1: Normal Lighting
        scenarios['Normal Lighting'] = self._create_normal_lighting_scenario()
        
        # Scenario 2: Low Light
        scenarios['Low Light'] = self._create_low_light_scenario()
        
        # Scenario 3: High Contrast
        scenarios['High Contrast'] = self._create_high_contrast_scenario()
        
        # Scenario 4: Noisy Scene
        scenarios['Noisy Scene'] = self._create_noisy_scenario()
        
        # Additional scenarios if requested
        if num_scenarios > 4:
            scenarios['Mixed Lighting'] = self._create_mixed_lighting_scenario()
            scenarios['Motion Blur'] = self._create_motion_blur_scenario()
            scenarios['Color Saturation'] = self._create_color_saturation_scenario()
            scenarios['Texture Rich'] = self._create_texture_rich_scenario()
        
        return scenarios
    
    def _create_normal_lighting_scenario(self) -> np.ndarray:
        """Create normal lighting test scenario"""
        height, width = 512, 512
        image = np.ones((height, width, 3), dtype=np.float32) * 0.5
        
        # Add geometric shapes for object detection
        cv2.rectangle(image, (50, 50), (150, 150), (0.8, 0.2, 0.2), -1)  # Red rectangle
        cv2.circle(image, (300, 100), 50, (0.2, 0.8, 0.2), -1)  # Green circle
        cv2.ellipse(image, (200, 300), (80, 40), 45, 0, 360, (0.2, 0.2, 0.8), -1)  # Blue ellipse
        
        # Add text-like regions
        cv2.rectangle(image, (20, 200), (120, 250), (0.9, 0.9, 0.9), -1)
        cv2.rectangle(image, (280, 200), (380, 250), (0.1, 0.1, 0.1), -1)
        
        # Add some edges for detection
        cv2.line(image, (100, 350), (200, 450), (1.0, 1.0, 1.0), 3)
        cv2.line(image, (300, 350), (400, 450), (0.0, 0.0, 0.0), 3)
        
        return image
    
    def _create_low_light_scenario(self) -> np.ndarray:
        """Create low light test scenario"""
        image = self._create_normal_lighting_scenario()
        # Reduce brightness significantly
        image = image * 0.25
        # Add noise typical of low light
        noise = np.random.normal(0, 0.05, image.shape)
        image = np.clip(image + noise, 0, 1)
        return image
    
    def _create_high_contrast_scenario(self) -> np.ndarray:
        """Create high contrast test scenario"""
        height, width = 512, 512
        image = np.zeros((height, width, 3), dtype=np.float32)
        
        # Create high contrast checkerboard pattern
        block_size = 32
        for i in range(0, height, block_size):
            for j in range(0, width, block_size):
                if (i // block_size + j // block_size) % 2 == 0:
                    image[i:i+block_size, j:j+block_size] = 1.0
        
        # Add some objects
        cv2.rectangle(image, (100, 100), (200, 200), (0.5, 0.5, 0.5), -1)
        cv2.circle(image, (350, 350), 60, (0.8, 0.3, 0.3), -1)
        
        return image
    
    def _create_noisy_scenario(self) -> np.ndarray:
        """Create noisy scene test scenario"""
        image = self._create_normal_lighting_scenario()
        # Add significant noise
        noise = np.random.normal(0, 0.1, image.shape)
        image = np.clip(image + noise, 0, 1)
        
        # Add some blur to simulate motion
        kernel = np.ones((5, 5), np.float32) / 25
        image = cv2.filter2D(image, -1, kernel)
        
        return image
    
    def _create_mixed_lighting_scenario(self) -> np.ndarray:
        """Create mixed lighting scenario"""
        image = self._create_normal_lighting_scenario()
        # Create gradient lighting
        height, width = image.shape[:2]
        gradient = np.linspace(0.3, 1.0, width).reshape(1, -1, 1)
        gradient = np.repeat(gradient, height, axis=0)
        gradient = np.repeat(gradient, 3, axis=2)
        image = image * gradient
        return image
    
    def _create_motion_blur_scenario(self) -> np.ndarray:
        """Create motion blur scenario"""
        image = self._create_normal_lighting_scenario()
        # Apply motion blur
        kernel = np.zeros((15, 15))
        kernel[7, :] = 1/15
        image = cv2.filter2D(image, -1, kernel)
        return image
    
    def _create_color_saturation_scenario(self) -> np.ndarray:
        """Create color saturation scenario"""
        image = self._create_normal_lighting_scenario()
        # Increase saturation
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.5, 0, 1)
        image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        return image
    
    def _create_texture_rich_scenario(self) -> np.ndarray:
        """Create texture-rich scenario"""
        image = self._create_normal_lighting_scenario()
        # Add texture patterns
        for i in range(0, 512, 64):
            for j in range(0, 512, 64):
                if (i + j) % 128 == 0:
                    cv2.rectangle(image, (i, j), (i+32, j+32), (0.7, 0.7, 0.7), 1)
        
        # Add noise for texture
        texture_noise = np.random.normal(0, 0.02, image.shape)
        image = np.clip(image + texture_noise, 0, 1)
        return image
    
    def evaluate_scenario(self, scenario_name: str, image: np.ndarray, 
                         iterations: int = 3) -> Dict[str, Any]:
        """Evaluate a single scenario with multiple iterations for statistical reliability"""
        
        traditional_scores = []
        adaptive_scores = []
        traditional_times = []
        adaptive_times = []
        applied_filters = []
        
        print(f"  Evaluating {scenario_name} ({iterations} iterations)...")
        
        for i in range(iterations):
            # Traditional ISP processing
            start_time = time.time()
            traditional_result = self.traditional_pipeline.process_image(image)
            traditional_time = time.time() - start_time
            
            # Adaptive ISP processing
            start_time = time.time()
            adaptive_result = self.adaptive_pipeline.process_image(image)
            adaptive_time = time.time() - start_time
            
            # Calculate detection scores
            traditional_detection = self.detection_analyzer.simulate_detection(traditional_result)
            adaptive_detection = self.detection_analyzer.simulate_detection(adaptive_result)
            
            traditional_scores.append(traditional_detection.detection_score)
            adaptive_scores.append(adaptive_detection.detection_score)
            traditional_times.append(traditional_time)
            adaptive_times.append(adaptive_time)
            
            # Record applied filters (same for all iterations in this demo)
            if i == 0:
                applied_filters = ['ExposureEnhancement', 'Gamma Correction', 'Denoising', 
                                 'Sharpening', 'ContrastEnhancement', 'Saturation enhancement']
        
        # Calculate statistics
        traditional_score_avg = statistics.mean(traditional_scores)
        adaptive_score_avg = statistics.mean(adaptive_scores)
        traditional_time_avg = statistics.mean(traditional_times)
        adaptive_time_avg = statistics.mean(adaptive_times)
        
        # Calculate performance improvement
        if traditional_score_avg > 0:
            performance_improvement = ((adaptive_score_avg - traditional_score_avg) / 
                                     traditional_score_avg) * 100
        else:
            performance_improvement = 0.0
        
        return {
            'scenario': scenario_name,
            'traditional_score': traditional_score_avg,
            'adaptive_score': adaptive_score_avg,
            'performance_improvement': performance_improvement,
            'traditional_time': traditional_time_avg,
            'adaptive_time': adaptive_time_avg,
            'applied_filters': applied_filters,
            'iterations': iterations,
            'traditional_scores': traditional_scores,
            'adaptive_scores': adaptive_scores,
            'traditional_times': traditional_times,
            'adaptive_times': adaptive_times
        }
    
    def generate_all_results(self, num_scenarios: int = 4, iterations: int = 3) -> Dict[str, Any]:
        """Generate comprehensive results for all scenarios"""
        
        print("=" * 60)
        print("Generating Quantitative Results for AdaptiveISP")
        print("=" * 60)
        
        # Create test scenarios
        print(f"\n1. Creating {num_scenarios} test scenarios...")
        scenarios = self.create_test_scenarios(num_scenarios)
        print(f"   Created {len(scenarios)} test scenarios")
        
        # Evaluate each scenario
        print(f"\n2. Evaluating scenarios ({iterations} iterations each)...")
        all_results = []
        
        for scenario_name, image in scenarios.items():
            result = self.evaluate_scenario(scenario_name, image, iterations)
            all_results.append(result)
            self.results.append(result)
            
            print(f"   {scenario_name}:")
            print(f"     Traditional ISP Score: {result['traditional_score']:.4f}")
            print(f"     Adaptive ISP Score: {result['adaptive_score']:.4f}")
            print(f"     Performance Improvement: {result['performance_improvement']:.2f}%")
            print(f"     Processing Time: {result['traditional_time']:.3f}s -> {result['adaptive_time']:.3f}s")
        
        # Generate statistical summary
        print(f"\n3. Generating statistical summary...")
        self.statistical_summary = self._generate_statistical_summary(all_results)
        
        # Save results
        print(f"\n4. Saving results to {self.output_dir}...")
        self._save_results(all_results, self.statistical_summary)
        
        # Generate visualizations
        print(f"\n5. Generating visualizations...")
        self._generate_visualizations(all_results)
        
        print(f"\n‚úÖ Quantitative results generation completed!")
        print(f"üìÅ Results saved to: {self.output_dir}")
        
        return {
            'scenario_results': all_results,
            'statistical_summary': self.statistical_summary,
            'output_directory': str(self.output_dir)
        }
    
    def _generate_statistical_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive statistical summary"""
        
        improvements = [r['performance_improvement'] for r in results]
        traditional_scores = [r['traditional_score'] for r in results]
        adaptive_scores = [r['adaptive_score'] for r in results]
        
        # Filter usage statistics
        all_filters = []
        for result in results:
            all_filters.extend(result['applied_filters'])
        
        filter_counts = {}
        for filter_name in all_filters:
            filter_counts[filter_name] = filter_counts.get(filter_name, 0) + 1
        
        total_filters = len(all_filters)
        filter_percentages = {name: (count / total_filters) * 100 
                            for name, count in filter_counts.items()}
        
        return {
            'performance_improvements': {
                'average': statistics.mean(improvements),
                'maximum': max(improvements),
                'minimum': min(improvements),
                'standard_deviation': statistics.stdev(improvements) if len(improvements) > 1 else 0,
                'median': statistics.median(improvements)
            },
            'traditional_scores': {
                'average': statistics.mean(traditional_scores),
                'maximum': max(traditional_scores),
                'minimum': min(traditional_scores),
                'standard_deviation': statistics.stdev(traditional_scores) if len(traditional_scores) > 1 else 0
            },
            'adaptive_scores': {
                'average': statistics.mean(adaptive_scores),
                'maximum': max(adaptive_scores),
                'minimum': min(adaptive_scores),
                'standard_deviation': statistics.stdev(adaptive_scores) if len(adaptive_scores) > 1 else 0
            },
            'filter_usage': {
                'counts': filter_counts,
                'percentages': filter_percentages,
                'total_applications': total_filters
            },
            'processing_times': {
                'traditional_avg': statistics.mean([r['traditional_time'] for r in results]),
                'adaptive_avg': statistics.mean([r['adaptive_time'] for r in results])
            }
        }
    
    def _save_results(self, results: List[Dict[str, Any]], summary: Dict[str, Any]):
        """Save results to JSON files"""
        
        # Save detailed results
        detailed_results_path = self.output_dir / "detailed_results.json"
        with open(detailed_results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Save statistical summary
        summary_path = self.output_dir / "statistical_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Save results table for easy viewing
        self._save_results_table(results)
        
        # Save filter usage report
        self._save_filter_usage_report(summary['filter_usage'])
    
    def _save_results_table(self, results: List[Dict[str, Any]]):
        """Save results in a formatted table"""
        
        table_path = self.output_dir / "results_table.md"
        
        with open(table_path, 'w') as f:
            f.write("# AdaptiveISP Quantitative Results\n\n")
            f.write("## Performance Comparison by Scenario\n\n")
            f.write("| Scenario | Traditional ISP Score | Adaptive ISP Score | Performance Improvement | Processing Time |\n")
            f.write("|----------|----------------------|-------------------|------------------------|-----------------|\n")
            
            for result in results:
                f.write(f"| {result['scenario']} | {result['traditional_score']:.4f} | "
                       f"{result['adaptive_score']:.4f} | {result['performance_improvement']:.2f}% | "
                       f"{result['traditional_time']:.3f}s ‚Üí {result['adaptive_time']:.3f}s |\n")
            
            f.write("\n## Statistical Summary\n\n")
            f.write(f"- **Average Performance Improvement**: {self.statistical_summary['performance_improvements']['average']:.2f}%\n")
            f.write(f"- **Maximum Performance Improvement**: {self.statistical_summary['performance_improvements']['maximum']:.2f}%\n")
            f.write(f"- **Minimum Performance Improvement**: {self.statistical_summary['performance_improvements']['minimum']:.2f}%\n")
            f.write(f"- **Performance Improvement Standard Deviation**: {self.statistical_summary['performance_improvements']['standard_deviation']:.2f}%\n")
            f.write(f"- **Average Traditional ISP Score**: {self.statistical_summary['traditional_scores']['average']:.4f}\n")
            f.write(f"- **Average Adaptive ISP Score**: {self.statistical_summary['adaptive_scores']['average']:.4f}\n")
    
    def _save_filter_usage_report(self, filter_usage: Dict[str, Any]):
        """Save filter usage statistics"""
        
        report_path = self.output_dir / "filter_usage_report.md"
        
        with open(report_path, 'w') as f:
            f.write("# Filter Usage Statistics\n\n")
            f.write("## Filter Usage Frequency\n\n")
            f.write("| Filter | Usage Count | Percentage |\n")
            f.write("|--------|-------------|------------|\n")
            
            for filter_name, count in filter_usage['counts'].items():
                percentage = filter_usage['percentages'][filter_name]
                f.write(f"| {filter_name} | {count} | {percentage:.1f}% |\n")
            
            f.write(f"\n**Total Filter Applications**: {filter_usage['total_applications']}\n")
    
    def _generate_visualizations(self, results: List[Dict[str, Any]]):
        """Generate comprehensive visualizations"""
        
        # Performance comparison chart
        self._create_performance_comparison_chart(results)
        
        # Processing time comparison
        self._create_processing_time_chart(results)
        
        # Filter usage pie chart
        self._create_filter_usage_chart(results)
        
        # Statistical distribution charts
        self._create_statistical_distributions(results)
    
    def _create_performance_comparison_chart(self, results: List[Dict[str, Any]]):
        """Create performance comparison visualization"""
        
        scenarios = [r['scenario'] for r in results]
        traditional_scores = [r['traditional_score'] for r in results]
        adaptive_scores = [r['adaptive_score'] for r in results]
        
        plt.figure(figsize=(12, 8))
        x = np.arange(len(scenarios))
        width = 0.35
        
        plt.bar(x - width/2, traditional_scores, width, label='Traditional ISP', alpha=0.8)
        plt.bar(x + width/2, adaptive_scores, width, label='Adaptive ISP', alpha=0.8)
        
        plt.xlabel('Test Scenarios')
        plt.ylabel('Detection Score')
        plt.title('Performance Comparison: Traditional ISP vs Adaptive ISP')
        plt.xticks(x, scenarios, rotation=45, ha='right')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for i, (trad, adapt) in enumerate(zip(traditional_scores, adaptive_scores)):
            plt.text(i - width/2, trad + 0.001, f'{trad:.3f}', ha='center', va='bottom')
            plt.text(i + width/2, adapt + 0.001, f'{adapt:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_processing_time_chart(self, results: List[Dict[str, Any]]):
        """Create processing time comparison chart"""
        
        scenarios = [r['scenario'] for r in results]
        traditional_times = [r['traditional_time'] for r in results]
        adaptive_times = [r['adaptive_time'] for r in results]
        
        plt.figure(figsize=(12, 6))
        x = np.arange(len(scenarios))
        width = 0.35
        
        plt.bar(x - width/2, traditional_times, width, label='Traditional ISP', alpha=0.8)
        plt.bar(x + width/2, adaptive_times, width, label='Adaptive ISP', alpha=0.8)
        
        plt.xlabel('Test Scenarios')
        plt.ylabel('Processing Time (seconds)')
        plt.title('Processing Time Comparison')
        plt.xticks(x, scenarios, rotation=45, ha='right')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'processing_time_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_filter_usage_chart(self, results: List[Dict[str, Any]]):
        """Create filter usage pie chart"""
        
        # Count filter usage
        filter_counts = {}
        for result in results:
            for filter_name in result['applied_filters']:
                filter_counts[filter_name] = filter_counts.get(filter_name, 0) + 1
        
        plt.figure(figsize=(10, 8))
        labels = list(filter_counts.keys())
        sizes = list(filter_counts.values())
        colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
        
        plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
        plt.title('Filter Usage Distribution')
        plt.axis('equal')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'filter_usage_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_statistical_distributions(self, results: List[Dict[str, Any]]):
        """Create statistical distribution charts"""
        
        improvements = [r['performance_improvement'] for r in results]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Performance improvement histogram
        axes[0, 0].hist(improvements, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_xlabel('Performance Improvement (%)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Performance Improvement Distribution')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Traditional vs Adaptive scores scatter plot
        traditional_scores = [r['traditional_score'] for r in results]
        adaptive_scores = [r['adaptive_score'] for r in results]
        axes[0, 1].scatter(traditional_scores, adaptive_scores, alpha=0.7, s=100)
        axes[0, 1].plot([0, max(max(traditional_scores), max(adaptive_scores))], 
                       [0, max(max(traditional_scores), max(adaptive_scores))], 'r--', alpha=0.8)
        axes[0, 1].set_xlabel('Traditional ISP Score')
        axes[0, 1].set_ylabel('Adaptive ISP Score')
        axes[0, 1].set_title('Traditional vs Adaptive ISP Scores')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Processing time comparison
        traditional_times = [r['traditional_time'] for r in results]
        adaptive_times = [r['adaptive_time'] for r in results]
        time_data = [traditional_times, adaptive_times]
        axes[1, 0].boxplot(time_data, labels=['Traditional ISP', 'Adaptive ISP'])
        axes[1, 0].set_ylabel('Processing Time (seconds)')
        axes[1, 0].set_title('Processing Time Distribution')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Performance improvement by scenario
        scenarios = [r['scenario'] for r in results]
        axes[1, 1].bar(scenarios, improvements, alpha=0.7, color='lightgreen')
        axes[1, 1].set_xlabel('Test Scenarios')
        axes[1, 1].set_ylabel('Performance Improvement (%)')
        axes[1, 1].set_title('Performance Improvement by Scenario')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'statistical_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()


def main():
    """Main function to generate quantitative results"""
    
    parser = argparse.ArgumentParser(description='Generate quantitative results for AdaptiveISP')
    parser.add_argument('--output_dir', type=str, default='quantitative_results',
                       help='Output directory for results')
    parser.add_argument('--scenarios', type=int, default=4,
                       help='Number of test scenarios to generate')
    parser.add_argument('--iterations', type=int, default=3,
                       help='Number of iterations per scenario for statistical reliability')
    
    args = parser.parse_args()
    
    # Create results generator
    generator = QuantitativeResultsGenerator(args.output_dir)
    
    # Generate comprehensive results
    results = generator.generate_all_results(
        num_scenarios=args.scenarios,
        iterations=args.iterations
    )
    
    # Print summary
    print("\n" + "=" * 60)
    print("QUANTITATIVE RESULTS SUMMARY")
    print("=" * 60)
    
    summary = results['statistical_summary']
    print(f"\nPerformance Improvements:")
    print(f"  Average: {summary['performance_improvements']['average']:.2f}%")
    print(f"  Maximum: {summary['performance_improvements']['maximum']:.2f}%")
    print(f"  Minimum: {summary['performance_improvements']['minimum']:.2f}%")
    print(f"  Standard Deviation: {summary['performance_improvements']['standard_deviation']:.2f}%")
    
    print(f"\nDetection Scores:")
    print(f"  Traditional ISP Average: {summary['traditional_scores']['average']:.4f}")
    print(f"  Adaptive ISP Average: {summary['adaptive_scores']['average']:.4f}")
    
    print(f"\nProcessing Times:")
    print(f"  Traditional ISP Average: {summary['processing_times']['traditional_avg']:.3f}s")
    print(f"  Adaptive ISP Average: {summary['processing_times']['adaptive_avg']:.3f}s")
    
    print(f"\nFilter Usage:")
    for filter_name, count in summary['filter_usage']['counts'].items():
        percentage = summary['filter_usage']['percentages'][filter_name]
        print(f"  {filter_name}: {count} times ({percentage:.1f}%)")
    
    print(f"\nüìÅ All results saved to: {results['output_directory']}")
    print("üìä Generated files:")
    print("  - detailed_results.json (complete data)")
    print("  - statistical_summary.json (summary statistics)")
    print("  - results_table.md (formatted table)")
    print("  - filter_usage_report.md (filter statistics)")
    print("  - performance_comparison.png (visualization)")
    print("  - processing_time_comparison.png (timing charts)")
    print("  - filter_usage_distribution.png (pie chart)")
    print("  - statistical_distributions.png (distribution plots)")


if __name__ == "__main__":
    main()
